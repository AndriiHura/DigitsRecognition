{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"model_weights_2attempt.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading some MNIST data for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "Y_train = train[\"label\"]\n",
    "X_train = train.drop(\"label\", axis=1)\n",
    "\n",
    "# Normalization\n",
    "test = test/255.\n",
    "X_train = X_train/255.\n",
    "\n",
    "X_train = np.array(X_train).reshape(-1, 28, 28, 1)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and accuracy estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated accuracy: 0.4811190476190476\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train)\n",
    "pred_classes = np.argmax(predictions,axis = 1) \n",
    "print(\"Estimated accuracy: {}\".format(accuracy_score(pred_classes, Y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAAAcCAAAAACcvWS2AAAB9UlEQVR4nGNgGAUDBmyXbFGngjFMqFzzp69tsaoTrY8U+P+AgGHyc8QIWsiCwlNZzMZwCZsy68z1y/qf/sRvFmsqMz5pZlNnWf4XKBaKrhRkePgRm2KF0ucuDPfw28cQwo1PVilbjoGBQRjZQq5lcmuDsXqQYSkDgwIhCzWEzzrjljXPZ727+tp35DhknmPQeY7hIi4dhCzkDF2AR1a9kPVA9dnvKImm03VOjwH2KCTGwuiNX3BL8hUxX5nxl4EB2cKC+HVVDAY4fSgg8P4DPvuMf+F0KgMDQ7DQ976/DAwMSKk0pGpvzn9u5SfvcGhRxO9BPvcePLIs9gx7XRyFn6w+A/ehzcQzib8Z9JlwOlMev4Vxy3/hkdXkZrCPkmRTKtODWaix8F7UdwYGfdxRiN+Hdo/u43OOMgPDt4kJRS8ZXWBBWs/Hd5uBgYGBoaIi8AhWPQoM+Iw01wpkYGBgYOhkmH8Ii7Qgw5uybwxfjwdwwiyMhFC3eOVxhAyT7M+neCzsZmBgYGAI9yh/hVX6P8O3bwwMDJoMV1DLUnHBe7hiQprt/n88FhIALxjknNkFE9Xf7EYtSzUYbmJTbtjMwMDAoLGFgaHwNnkWHg7ly8hgYPje/w3VQnXsFp73Ic8aBPhaF63C8+7cpjeUGjQKRsEoGJIAAM8XhtMdW+TJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=112x28 at 0x1EBA4531A88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image_1 = Image.open(\"images/00000.png\")\n",
    "display(test_image_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 28)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_1.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_test_image_1 = np.array(test_image_1, dtype = 'float32').reshape(1,28,112,1)\n",
    "array_test_image_1 = array_test_image_1 / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 112)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_image_1,dtype = 'float32').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 112, 1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_test_image_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ebdf580f48>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(array_test_image_1[0][:,:28,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting splitted image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1eba11a9448>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(array_test_image_1[0][:,28:56,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(array_test_image_1[:, :,28:56,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_test_image_1[:, :,:28,].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to generate sample like data(composed from mnist numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_1x4_image(num_of_images, num_of_digits):\n",
    "    list_of_images = []\n",
    "    actual_classes = []\n",
    "    for im in range(num_of_images):\n",
    "        rand_indexes = np.random.randint(28000, size=(num_of_digits))\n",
    "        long_image  = X_train[rand_indexes[0]][:,:,0]\n",
    "        image_labels =[ Y_train[index] for index in rand_indexes]\n",
    "        for ind in range(1,num_of_digits):\n",
    "            long_image  = np.concatenate((long_image, X_train[rand_indexes[ind]][:,:,0]), axis=1)\n",
    "        actual_classes.append(image_labels)\n",
    "        list_of_images.append(long_image)\n",
    "    return np.array(list_of_images), actual_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to create some test data, then run this cell\n",
    "# Pass to the function number of test images, and amount of numbers each image will contain\n",
    "gen_images, gen_images_labels = generate_1x4_image(20, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 28, 112)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving generated images to the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for im in range(20):\n",
    "    save_image_name = \"\".join(list(map(lambda x: str(x), gen_images_labels[im])))\n",
    "    save_image = (((gen_images[im] - gen_images[im].min()) /\\\n",
    "                   (gen_images[im].max() - gen_images[im].min())) * 255.9).astype(np.uint8)\n",
    "    save_image = Image.fromarray(save_image)\n",
    "    save_path = \"GeneratedTestImages/\"+save_image_name+\".png\"\n",
    "    save_image.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to split whole image, and to predict whole image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(image_array, num_of_digits):\n",
    "    \"\"\"Function splits the whole image into several images - 1 image for each digit\n",
    "       Takes: image_array - (1, 28, 28*num_of_digits, 1)\n",
    "       returns (num_of_images, 28, 28, 1) array\"\"\"\n",
    "    \n",
    "    new_array = image_array.copy().reshape(num_of_digits, 28, 28, 1)\n",
    "    for im in range(num_of_digits):\n",
    "        new_array[im][:,:,0] = image_array[0][:,28*im:28*(im+1), 0]# separating imput image into N images\n",
    "    return new_array\n",
    "\n",
    "\n",
    "def predict_full_image(image_path):\n",
    "    \"\"\"Function predicts all numbers on input image, in order from left to right\"\"\"\n",
    "    \n",
    "    # Openning image\n",
    "    input_image = Image.open(image_path)\n",
    "    # Converting it into numpy array, each value is in range (0, 255)\n",
    "    image_array = np.array(input_image, dtype = 'float32')\n",
    "    # Normalizing image\n",
    "    image_array = image_array / 255.\n",
    "    image_shape = image_array.shape\n",
    "    if(image_shape[0] > image_shape[1]): # Image is looks like column of numbers\n",
    "        image_array = np.rot90(image_array)\n",
    "        image_shape = image_array.shape\n",
    "    # Reshaping from (28, 28*num_digits) to (1, 28, 28*num_digits,1)\n",
    "    image_array = image_array.reshape(1, 28, image_shape[1],1)\n",
    "    num_of_digits = image_shape[1] // 28\n",
    "    # Reshaping from (1,28, 28*num_digits,1) to (num_digits, 28, 28,1)\n",
    "    splitted_images = split_image(image_array, num_of_digits)\n",
    "    # Predicting result\n",
    "    answer = model.predict_classes(splitted_images)\n",
    "    answer = \"\".join(str(x) for x in answer)\n",
    "    return(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9985'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_full_image(\"C:/Users/AndriiHura/Desktop/4973.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4770'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_full_image(\"images/00009.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_images = os.listdir(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4146\n",
      "2978\n",
      "3161\n",
      "2796\n",
      "8129\n",
      "6528\n",
      "2176\n",
      "2407\n",
      "6851\n",
      "4770\n"
     ]
    }
   ],
   "source": [
    "for image in example_images:\n",
    "    path = \"images/\"+image\n",
    "    print(predict_full_image(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image in example_images:\n",
    "#     path = \"images/\"+image\n",
    "#     plt.imshow((path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_images(example_images):\n",
    "    images_array = np.zeros((40, 28, 28, 1))\n",
    "    i = 0\n",
    "    for image in example_images:\n",
    "        path = \"images/\"+image\n",
    "        # Openning image\n",
    "        input_image = Image.open(path)\n",
    "        # Converting it into numpy array, each value is in range (0, 255)\n",
    "        image_array = np.array(input_image, dtype = 'float32')\n",
    "        # Normalizing image\n",
    "        image_array = image_array / 255.\n",
    "        image_shape = image_array.shape\n",
    "        if(image_shape[0] > image_shape[1]): # Image is looks like column of numbers\n",
    "            image_array = np.rot90(image_array)\n",
    "            image_shape = image_array.shape\n",
    "        # Reshaping from (28, 28*num_digits) to (1, 28, 28*num_digits,1)\n",
    "        image_array = image_array.reshape(1, 28, image_shape[1],1)\n",
    "        num_of_digits = image_shape[1] // 28\n",
    "        # Reshaping from (1,28, 28*num_digits,1) to (num_digits, 28, 28,1)\n",
    "        splitted_images = split_image(image_array, num_of_digits)\n",
    "        images_array[i:i+4, :, :, :] = splitted_images\n",
    "        i += 4\n",
    "    return images_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_images_array = get_all_images(example_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 28, 28, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_images_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_images_labels = np.array([4, 1, 4, 6,   \n",
    "                                  2, 9, 7, 8,   \n",
    "                                  3, 1, 6, 1,   \n",
    "                                  2, 7, 9, 6,  \n",
    "                                  8, 1, 2, 9,  \n",
    "                                  6, 5, 2, 8,  \n",
    "                                  2, 1, 7, 6,\n",
    "                                  2, 4, 0, 7,\n",
    "                                  6, 8, 5, 1,\n",
    "                                  4, 7, 7, 0\n",
    "                                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(example_images_array[5, :, :, 0])\n",
    "print(example_images_labels[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 4, 1: 6, 6: 6, 2: 6, 9: 3, 7: 6, 8: 4, 3: 1, 5: 2, 0: 2})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(example_images_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.2, # Randomly zoom image \n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=True)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ImageDataGenerator.flow of <keras.preprocessing.image.ImageDataGenerator object at 0x000001EBA4535D48>>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagen.flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\" , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ReduceLROnPlateau' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-5cf9e16b917d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n\u001b[0m\u001b[0;32m      2\u001b[0m                                             \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                             \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                             min_lr=0.00001)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ReduceLROnPlateau' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_4 to have shape (10,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-9e44014370de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n\u001b[1;32m----> 2\u001b[1;33m                     steps_per_epoch=len(X_train) / 32, epochs=5)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                                             reset_metrics=False)\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1506\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1507\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1508\u001b[1;33m             class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1509\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_4 to have shape (10,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n",
    "                    steps_per_epoch=len(X_train) / 32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
