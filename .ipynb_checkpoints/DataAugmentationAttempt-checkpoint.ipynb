{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(image_array, num_of_digits):\n",
    "    \"\"\"Function splits the whole image into several images - 1 image for each digit\n",
    "       Takes: image_array - (1, 28, 28*num_of_digits, 1)\n",
    "       returns (num_of_images, 28, 28, 1) array\"\"\"\n",
    "    \n",
    "    new_array = image_array.copy().reshape(num_of_digits, 28, 28, 1)\n",
    "    for im in range(num_of_digits):\n",
    "        new_array[im][:,:,0] = image_array[0][:,28*im:28*(im+1), 0]# separating imput image into N images\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_images(example_images):\n",
    "    images_array = np.zeros((40, 28, 28, 1))\n",
    "    i = 0\n",
    "    for image in example_images:\n",
    "        path = \"images/\"+image\n",
    "        # Openning image\n",
    "        input_image = Image.open(path)\n",
    "        # Converting it into numpy array, each value is in range (0, 255)\n",
    "        image_array = np.array(input_image, dtype = 'float32')\n",
    "        # Normalizing image\n",
    "        image_array = image_array / 255.\n",
    "        image_shape = image_array.shape\n",
    "        if(image_shape[0] > image_shape[1]): # Image is looks like column of numbers\n",
    "            image_array = np.rot90(image_array)\n",
    "            image_shape = image_array.shape\n",
    "        # Reshaping from (28, 28*num_digits) to (1, 28, 28*num_digits,1)\n",
    "        image_array = image_array.reshape(1, 28, image_shape[1],1)\n",
    "        num_of_digits = image_shape[1] // 28\n",
    "        # Reshaping from (1,28, 28*num_digits,1) to (num_digits, 28, 28,1)\n",
    "        splitted_images = split_image(image_array, num_of_digits)\n",
    "        images_array[i:i+4, :, :, :] = splitted_images\n",
    "        i += 4\n",
    "    return images_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_images = os.listdir(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = get_all_images(example_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 28, 28, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(  [4, 1, 4, 6,   \n",
    "                      2, 9, 7, 8,   \n",
    "                      3, 1, 6, 1,   \n",
    "                      2, 7, 9, 6,  \n",
    "                      8, 1, 2, 9,  \n",
    "                      6, 5, 2, 8,  \n",
    "                      2, 1, 7, 6,\n",
    "                      2, 4, 0, 7,\n",
    "                      6, 8, 5, 1,\n",
    "                      4, 7, 7, 0\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(Y_train, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAANa0lEQVR4nO3df6zd9V3H8deLcumWwkzvWLtScIO2oJVoN69lE2JQ4lJwpCUGXA1QEmbnXCMsGIdbFBKjNsCci05MkabVMAgR2IgSbW3mEKWVW1ZpS0FK10HptQW7SYtS+uPtH/eLucA9n3t7vt/zY7yfj+TknPN9n8/5vnt6Xvf7Ped7zvk4IgTg3e+kXjcAoDsIO5AEYQeSIOxAEoQdSOLkbq7sFE+N92haN1cJpPK6XtMbcdjj1WqF3fYiSV+VNEXSX0bEytLt36NpusCX1FklgIJNsaFlre3deNtTJH1N0qWS5ktaant+u/cHoLPqvGZfKGlnROyKiDck3SdpcTNtAWhanbDPlvTimOt7qmVvYXu57WHbw0d0uMbqANRRJ+zjvQnwjs/eRsSqiBiKiKEBTa2xOgB11An7Hklnjbl+pqS99doB0Cl1wv6EpHm2z7Z9iqRPSXq4mbYANK3tQ28RcdT2Ckn/oNFDb6sjYntjnQFoVK3j7BHxiKRHGuoFQAfxcVkgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAErWmbLa9W9JBScckHY2IoSaaAtC8WmGv/HxEvNLA/QDoIHbjgSTqhj0krbO92fby8W5ge7ntYdvDR3S45uoAtKvubvyFEbHX9gxJ620/ExGPjr1BRKyStEqS3ufBqLk+AG2qtWWPiL3V+X5JD0la2ERTAJrXdthtT7N92puXJX1C0ramGgPQrDq78TMlPWT7zfv5ekT8fSNd4S0O/9LPFOvHf7P1wZA7zr2/OPa8gaPF+t8cPLtY/6NHlhTr8760pWXt+OuvF8eiWW2HPSJ2SfqpBnsB0EEcegOSIOxAEoQdSIKwA0kQdiCJJr4Ig5p23fbxYv25q+8s1uf+03Uta7dcfk1xrF8+UKy/eM3cYn3jDbcX69d+9MrWxV8YKY5V8IHLJrFlB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkOM7eBd+/rt5x9PP++dpifc6vFr5GWhw5sTPu2F+sf3zGbxXrpX/bwmWfLY6dvubxYh0nhi07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBcfYu+OWb/rFYPxblo+Fzfu9/yuNPuKPmnP1QuTdd3bp06JMHi0OnrznxftAaW3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSILj7A2YMrc8rfEX3v9QsX73f59RrB97ducJ99QtAyPfb3vsBWd+r1jf2/Y9YzwTbtltr7a93/a2McsGba+3/Vx1Pr2zbQKoazK78WskLXrbspslbYiIeZI2VNcB9LEJwx4Rj0p6+xxBiyWtrS6vlbSk4b4ANKzdN+hmRsSIJFXnM1rd0PZy28O2h4/ocJurA1BXx9+Nj4hVETEUEUMDmtrp1QFood2w77M9S5Kq8/JPkALouXbD/rCkZdXlZZK+2Uw7ADplwuPstu+VdLGk023vkXSLpJWS7rd9vaQXJBUm4X73+9+5p9ca/+0fnDvBLV6tdf+d9Po5H2h77LFwg51gIhOGPSKWtihd0nAvADqIj8sCSRB2IAnCDiRB2IEkCDuQBF9xbcDJh47UGv+j7y1/TXSfptS6/0767pL2n0LPHJhZrE/v40OOP4zYsgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEhxnb8CUjduK9W+8dmqx/uuDjxfr35l9VbF+9KXO/ejylPnlr9/ed/mfTXAPAy0rr+weLI7kJ4ubxZYdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5LgOHsD4ujRYn3lrVcX69++7U+L9ffe90axfujz57esnbR7pDh2/5LycfRFKx4r1j//7K8U6//ykw+2rM34V35KupvYsgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEhxn74IfuWdjsX7py58t1uf8/o5i/U8e/IuWtYPHy3/PP/1M+TMAm6/9iWL9Z9c8W6y/cuy1lrXBvyv/u44VqzhRE27Zba+2vd/2tjHLbrX9ku0t1emyzrYJoK7J7MavkbRonOVfiYgF1emRZtsC0LQJwx4Rj0o60IVeAHRQnTfoVth+qtrNb/lzYbaX2x62PXxEh2usDkAd7Yb9TklzJC2QNCLpy61uGBGrImIoIoYGNLXN1QGoq62wR8S+iDgWEccl3SVpYbNtAWhaW2G3PWvM1SsklX9LGUDPTXic3fa9ki6WdLrtPZJukXSx7QWSQtJuSZ/pYI/vegPrhov1F9aVx1+vi9pe96naVb7BgvnF8u0f/E6xfv7G5S1rs3+wvbxuNGrCsEfE0nEW392BXgB0EB+XBZIg7EAShB1IgrADSRB2IAm+4oqiXb9TfoocjiPF+hm38xTrF2zZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJDoJO0smzPtiydsY3DhbHbtz7oWL9jCuebqunJhy66mPF+tMX/XmxPu+BFeX645tOuCd0Blt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC4+yTdHTkP1vWNu87rzh26wVfL9bP/cPylM1zv/p8se6TWv/N/u6nzymOXfdrtxXr8x/7jWJ93g3/Vqyjf7BlB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkHBFdW9n7PBgX+JKura9bPHBKsf78H/x0sX7jJ/+2WL/ytGeK9YPHW/8f/u6ey4tjd971Y8X69DWPF+voL5tig16NAx6vNuGW3fZZtr9le4ft7bZvqJYP2l5v+7nqfHrTjQNozmR2449KuikiflzSxyR9zvZ8STdL2hAR8yRtqK4D6FMThj0iRiLiyeryQUk7JM2WtFjS2upmayUt6VSTAOo7oTfobH9Y0kckbZI0MyJGpNE/CJJmtBiz3Paw7eEjOlyvWwBtm3TYbZ8q6QFJN0bEq5MdFxGrImIoIoYGNLWdHgE0YFJhtz2g0aDfExEPVov32Z5V1WdJ2t+ZFgE0YcJDb7at0dfkByLixjHLb5f0XxGx0vbNkgYj4rdL9/VuPfQG9IvSobfJfJ/9QknXSNpqe0u17IuSVkq63/b1kl6QdGUTzQLojAnDHhGPSRr3L4UkNtPADwk+LgskQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASE4bd9lm2v2V7h+3ttm+olt9q+yXbW6rTZZ1vF0C7JjM/+1FJN0XEk7ZPk7TZ9vqq9pWIuKNz7QFoymTmZx+RNFJdPmh7h6TZnW4MQLNO6DW77Q9L+oikTdWiFbafsr3a9vQWY5bbHrY9fESHazULoH2TDrvtUyU9IOnGiHhV0p2S5khaoNEt/5fHGxcRqyJiKCKGBjS1gZYBtGNSYbc9oNGg3xMRD0pSROyLiGMRcVzSXZIWdq5NAHVN5t14S7pb0o6I+OMxy2eNudkVkrY13x6Apkzm3fgLJV0jaavtLdWyL0paanuBpJC0W9JnOtIhgEZM5t34xyR5nNIjzbcDoFP4BB2QBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJR0T3Vma/LOl7YxadLumVrjVwYvq1t37tS6K3djXZ24ci4gPjFboa9nes3B6OiKGeNVDQr731a18SvbWrW72xGw8kQdiBJHod9lU9Xn9Jv/bWr31J9NaurvTW09fsALqn11t2AF1C2IEkehJ224tsP2t7p+2be9FDK7Z3295aTUM93ONeVtveb3vbmGWDttfbfq46H3eOvR711hfTeBemGe/pY9fr6c+7/prd9hRJ/yHpFyXtkfSEpKUR8XRXG2nB9m5JQxHR8w9g2P45SYck/VVEnF8tu03SgYhYWf2hnB4RX+iT3m6VdKjX03hXsxXNGjvNuKQlkq5TDx+7Ql9XqQuPWy+27Asl7YyIXRHxhqT7JC3uQR99LyIelXTgbYsXS1pbXV6r0SdL17XorS9ExEhEPFldPijpzWnGe/rYFfrqil6EfbakF8dc36P+mu89JK2zvdn28l43M46ZETEijT55JM3ocT9vN+E03t30tmnG++axa2f687p6EfbxppLqp+N/F0bERyVdKulz1e4qJmdS03h3yzjTjPeFdqc/r6sXYd8j6awx18+UtLcHfYwrIvZW5/slPaT+m4p635sz6Fbn+3vcz//rp2m8x5tmXH3w2PVy+vNehP0JSfNsn237FEmfkvRwD/p4B9vTqjdOZHuapE+o/6aifljSsuryMknf7GEvb9Ev03i3mmZcPX7sej79eUR0/STpMo2+I/+8pC/1oocWfZ0j6d+r0/Ze9ybpXo3u1h3R6B7R9ZLeL2mDpOeq88E+6u2vJW2V9JRGgzWrR71dpNGXhk9J2lKdLuv1Y1foqyuPGx+XBZLgE3RAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kMT/AXBEDFf424/cAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[5, :, :, 0])\n",
    "print(Y_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\" , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.2, # Randomly zoom image \n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 10)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 0s - loss: 2.1930 - accuracy: 0.1500\n",
      "Epoch 2/50\n",
      " - 0s - loss: 2.1670 - accuracy: 0.1750\n",
      "Epoch 3/50\n",
      " - 0s - loss: 2.2095 - accuracy: 0.1500\n",
      "Epoch 4/50\n",
      " - 0s - loss: 2.2163 - accuracy: 0.1750\n",
      "Epoch 5/50\n",
      " - 0s - loss: 2.1804 - accuracy: 0.1500\n",
      "Epoch 6/50\n",
      " - 0s - loss: 2.2059 - accuracy: 0.1500\n",
      "Epoch 7/50\n",
      " - 0s - loss: 2.2118 - accuracy: 0.1125\n",
      "Epoch 8/50\n",
      " - 0s - loss: 2.2088 - accuracy: 0.1750\n",
      "Epoch 9/50\n",
      " - 0s - loss: 2.1895 - accuracy: 0.1750\n",
      "Epoch 10/50\n",
      " - 0s - loss: 2.2165 - accuracy: 0.1125\n",
      "Epoch 11/50\n",
      " - 0s - loss: 2.1818 - accuracy: 0.1500\n",
      "Epoch 12/50\n",
      " - 0s - loss: 2.1820 - accuracy: 0.1375\n",
      "Epoch 13/50\n",
      " - 0s - loss: 2.1884 - accuracy: 0.1875\n",
      "Epoch 14/50\n",
      " - 0s - loss: 2.1998 - accuracy: 0.1000\n",
      "Epoch 15/50\n",
      " - 0s - loss: 2.1698 - accuracy: 0.2125\n",
      "Epoch 16/50\n",
      " - 0s - loss: 2.2170 - accuracy: 0.1500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=600),\n",
    "                              epochs = epochs,\n",
    "                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 8576/60000 [===>..........................] - ETA: 4:25 - loss: 0.5843 - accuracy: 0.8084"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-0bd8b6cfd5b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m history = model.fit(train_images,train_labels, batch_size=32,\n\u001b[1;32m----> 2\u001b[1;33m                         epochs = epochs, verbose = 1, callbacks=[learning_rate_reduction])\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images,train_labels, batch_size=32,\n",
    "                        epochs = epochs, verbose = 1, callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
